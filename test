# =========================================================
# JUPYTER NOTEBOOK â€“ FULL WORKING EXAMPLE (FINAL)
# =========================================================

import asyncio
from typing import List

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.prompt_values import ChatPromptValue

from agent_framework import ChatAgent
from agent_framework.messages import ChatMessage, Role


# =========================================================
# ORG STANDARD MODEL FACTORY
# =========================================================
def model(version: str):
    return MyCustomLLM(version)


# =========================================================
# CUSTOM LLM (ACCEPTS ChatPromptValue ONLY)
# =========================================================
class MyCustomLLM:
    def __init__(self, version: str):
        self.version = version

    def invoke(self, prompt_value: ChatPromptValue) -> ChatMessage:
        print(f"\nðŸ”µ LLM model({self.version}) INVOKED WITH PROMPT:\n")
        print(prompt_value.to_string())

        return ChatMessage(
            role=Role.ASSISTANT,
            content="Hello ðŸ‘‹ This response came from model('3')"
        )


# =========================================================
# CUSTOM CHAT CLIENT (FRAMEWORK COMPATIBLE)
# =========================================================
class MyChatClient:
    """
    Required by ChatAgent:
    - get_response()
    """

    async def get_response(
        self,
        messages: List[ChatMessage],
        *,
        tools=None,
        **kwargs,
    ) -> ChatMessage:
        return await self.invoke(messages, tools=tools, **kwargs)

    async def invoke(
        self,
        messages: List[ChatMessage],
        *,
        tools=None,
        **kwargs,
    ) -> ChatMessage:

        # Convert ChatMessages â†’ ChatPromptTemplate
        prompt_template = ChatPromptTemplate.from_messages(
            [(m.role, m.content) for m in messages]
        )

        # Convert â†’ ChatPromptValue
        prompt_value = prompt_template.format_prompt()

        # ORG STANDARD MODEL CALL
        llm = model("3")
        response_message = llm.invoke(prompt_value)

        return response_message


# =========================================================
# GREETING AGENT
# =========================================================
class GreetingAgent(ChatAgent):
    def __init__(self):
        super().__init__(
            name="GreetingAgent",
            chat_client=MyChatClient(),
            instructions="You are a friendly greeting assistant.",
        )


# =========================================================
# RUN IN NOTEBOOK
# =========================================================
async def main():
    agent = GreetingAgent()
    response = await agent.run("Hi")

    print("\nðŸŸ¢ FINAL RESPONSE TEXT:\n")
    # âœ… CORRECT WAY TO PRINT OUTPUT
    print(response.messages[0].content.value)

await main()
