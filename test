import uuid
import time
import copy
from typing import Dict, List, Any
from app.chunking.automerger import AutoMergeChunker
from aida_genai_core.nlp.remove_pii import remove_pii
from app.utils.elf_logging import logger


class Chunking:
    def __init__(self, producer_utils=None):
        self.auto_merger = AutoMergeChunker()
        self.all_chunks_child = []
        self.hyde_chunks_store = []
        self.separators = ['\\\n\\\n', '\\\n', ' ', '.', '?', '!', ',', ';', ':']
        self.producer_utils = producer_utils

    def create_chunks(self, data: Dict[str, List[Dict[str, Any]]], chunking: bool = True) -> List[Dict[str, Any]]:
        try:
            for filename, pages in data.items():
                if chunking:
                    chunks = self._process_document(pages, filename)
                else:
                    chunks = []
                    for page in pages:
                        doc_id = str(uuid.uuid4())
                        md = page.metadata.copy()
                        md.update({
                            "doc_id": doc_id,
                            "file_name": filename,
                            "level": 2,
                            "type": "doc",
                            "parent_node": None,
                            "prev_node": None,
                            "next_node": None,
                            "child_nodes": None,
                            "chunking_type": "No chunking",
                        })
                        chunks.append({
                            "content": page.page_content,
                            "metadata": md,
                        })

                self.all_chunks_child.extend(self._prepare_datastax_chunks(chunks))

            logger.info(f"Created {len(self.all_chunks_child)} chunks ({'chunked' if chunking else 'un-chunked'})")
            return self.all_chunks_child

        except Exception as e:
            logger.error(f"Error in chunk creation: {e}")
            raise

    def _prepare_datastax_chunks(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        return [
            {
                'content': chunk['content'],
                'metadata': {
                    k: str(v) if not isinstance(v, (str, int, float, bool)) else v
                    for k, v in chunk['metadata'].items()
                }
            }
            for chunk in chunks if chunk['metadata'].get('level') == 2
        ]

    def _process_document(self, pages: List[Dict[str, Any]], filename: str) -> List[Dict[str, Any]]:
        return self.auto_merger._create_chunks(pages)

    def automerger_chunking(self, docs: Dict[str, List[Dict[str, Any]]], message: Dict[str, Any]) -> int:
        chunks = self.create_chunks(data=docs, chunking=True)
        file_name = chunks[0]["metadata"].get("source", "unknown")
        logger.info(f"Processed {file_name}: {len(chunks)} chunks")

        batch = []
        batch_size = 5

        for i, chunk in enumerate(chunks):
            metadata = chunk.get("metadata", {})
            data = {
                "type": metadata.get("type", ""),
                "text": chunk.get("content", ""),
                "chunked_content": chunk.get("content", ""),
                "chunked_content_clean": remove_pii(str(chunk.get("content", ""))),
                "doc_id": metadata.get("doc_id", ""),
                "filename": metadata.get("source", ""),
                "filetype": metadata.get("type", ""),
                "parent_node": metadata.get("parent_node", ""),
                "prev_node": metadata.get("prev_node", ""),
                "level": metadata.get("level", ""),
                "child_nodes": str(metadata.get("child_nodes", "")),
                "next_node": metadata.get("next_node", ""),
                "source": metadata.get("source", ""),
                "period": metadata.get("period", ""),
                "date": metadata.get("date", ""),
                "heading": metadata.get("heading", ""),
                "subheading": metadata.get("subheading", ""),
                "table": metadata.get("table", ""),
                "image_context": metadata.get("image_context", ""),  # no base64
                "tmrc_checklist": metadata.get("tmrc_checklist", ""),
                "section": metadata.get("section", ""),
                "subsection": metadata.get("subsection", ""),
                "caption": metadata.get("caption", ""),
                "style": metadata.get("style", ""),
                "style_extracted": metadata.get("style_extracted", ""),
                "company_name": metadata.get("company_name", ""),
                "start_index": 0,
                "end_index": len(chunk.get("content", "")),
                "doc_format": "docx",
                "input_type": "doc"
            }

            batch.append(data)

            if (i + 1) % batch_size == 0 or (i + 1) == len(chunks):
                try:
                    message_copy = copy.deepcopy(message)
                    message_copy["total_chunks"] = len(chunks)
                    message_copy["chunk_no"] = i + 1
                    message_copy["input_data"] = batch
                    self.producer_utils.publish_message(message_copy)
                    logger.info(f"Published chunk batch {i + 1}/{len(chunks)}")
                    time.sleep(0.2)  # Throttle to reduce CPU spike
                except Exception as e:
                    logger.exception(f"Failed to publish chunk batch {i + 1}: {str(e)}")
                batch = []

        return len(chunks)
import configparser
import os
import threading
import traceback
import time

from fastapi import FastAPI
from jproperties import Properties
from sqlalchemy import create_engine
from sqlalchemy.orm import sessionmaker
from starlette.middleware.cors import CORSMiddleware

from app.utils.elf_logging import logger
from app.utils.kafka_consumer import consume_messages
from app.utils.kafka_producer import KafkaProducerUtils
from app.utils.string_template_query_utils import StringTemplateQueryUtils

app = FastAPI()

# CORS settings
app.add_middleware(
    CORSMiddleware,
    allow_origin_regex=".*",
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Safe wrapper for each Kafka thread to catch exceptions
def consume_thread_safe(i, config, secrets, env, producer, query_utils):
    try:
        consume_messages(i, config, secrets, env, producer, query_utils)
    except Exception as e:
        logger.exception(f"Kafka consumer thread {i} crashed: {e}")

@app.on_event("startup")
async def startup_event():
    try:
        # Load config and secrets
        config = configparser.ConfigParser()
        config.read(r"resources/application.properties")

        secrets = Properties()
        with open("secrets_folder/secrets", 'rb') as config_file:
            secrets.load(config_file)

        env = secrets.get("ENV").data

        # Set environment variables
        os.environ['S3_REGION_NAME'] = secrets.get('S3_REGION_NAME').data
        os.environ['S3_BUCKET_NAME'] = secrets.get('S3_BUCKET_NAME').data
        os.environ['S3_ACCESS_KEY'] = secrets.get('S3_ACCESS_KEY').data
        os.environ['S3_SECRET_KEY'] = secrets.get('S3_SECRET_KEY').data
        os.environ["CONSUMER_INTEGRATION_ID"] = secrets.get("CONSUMER_INTEGRATION_ID").data
        os.environ["CONSUMER_SECRET"] = secrets.get("CONSUMER_SECRET").data
        os.environ["CONFIG_PATH"] = f"resources/safechain_config_{env.lower()}.yml"

        # Kafka producer
        kafka_producer = KafkaProducerUtils(config, secrets, env)
        app.state.kafka_producer = kafka_producer

        # Postgres DB engine
        database_url = f"postgresql+psycopg2://{secrets.get('POSTGRES_DB_USERNAME').data}:{secrets.get('POSTGRES_DB_PASSWORD').data}@{config[env]['HOSTNAME']}:5432/{config[env]['DATABASE_NAME']}"
        logger.info("Creating Postgres Engine...")
        engine = create_engine(
            database_url,
            pool_size=15,
            max_overflow=2,
            pool_recycle=1800,
            pool_pre_ping=True,
            pool_timeout=30
        )
        app.state.engine = engine
        logger.info("Postgres Engine created successfully")

        # SQLAlchemy session utility
        session_local = sessionmaker(autocommit=False, autoflush=False, bind=engine)
        string_template_query_utils = StringTemplateQueryUtils(session_local)

        # Start Kafka consumer threads
        threads = []
        consumer_group_size = int(config[env]["CONSUMER_GROUP_SIZE"])
        logger.info(f"Starting {consumer_group_size} Kafka consumers...")
        for i in range(consumer_group_size):
            thread = threading.Thread(
                target=consume_thread_safe,
                args=(i, config, secrets, env, kafka_producer, string_template_query_utils),
                daemon=True  # Optional: set daemon so it exits with app
            )
            threads.append(thread)
            thread.start()
            logger.info(f"Consumer thread {i} started.")
            time.sleep(0.2)  # Prevent CPU spike by spacing thread startup

    except Exception as e:
        logger.exception("Startup failed. Application will not proceed.")
        raise  # Ensures FastAPI doesn't falsely show READY

@app.get("/")
async def home():
    return {"response": "Home Page"}

@app.get("/health")
async def health_check():
    return {"status": "ok"}

@app.on_event("shutdown")
async def shutdown_event():
    logger.warning("Shutdown triggered â€” beginning cleanup...")
    logger.debug("Shutdown stack trace:\n" + "".join(traceback.format_stack()))

    if hasattr(app.state, "kafka_producer"):
        try:
            app.state.kafka_producer.close_kafka_connection()
            logger.info("Kafka Producer closed on shutdown")
        except Exception as e:
            logger.error(f"Error closing Kafka producer: {e}")

    if hasattr(app.state, "engine"):
        try:
            app.state.engine.dispose()
            logger.info("SQLAlchemy engine disposed cleanly on shutdown")
        except Exception as e:
            logger.error(f"Error disposing engine: {e}")
