from sklearn.feature_extraction.text import TfidfVectorizer

# Sample tokenization function
def tokenize(texts):
    return [" ".join(text.split()) for text in texts]

# Tokenize both reference and test datasets
X_ref_tokenized = tokenize(X_ref)  # Replace X_ref with your reference data
X_test_tokenized = tokenize(X_test)  # Replace X_test with your test data

# TF-IDF Vectorization
vectorizer = TfidfVectorizer(max_features=10000)

# Fit on reference data
X_ref_tfidf = vectorizer.fit_transform(X_ref_tokenized).toarray()

# Transform test data using the same vectorizer
X_test_tfidf = vectorizer.transform(X_test_tokenized).toarray()

# Validate dimensions
assert X_ref_tfidf.shape[1] == X_test_tfidf.shape[1], "Feature dimensions do not match!"

# Ready for drift detection or further analysis
print(f"X_ref shape: {X_ref_tfidf.shape}, X_test shape: {X_test_tfidf.shape}")
