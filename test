Encode the Text: Use the [CLS] token embeddings as a representation of the entire text. This provides a fixed-length feature vector for each text.

Compute Pairwise Distances:

Use metrics like Cosine Similarity or Euclidean Distance to compute the distance between embeddings from the training and new data.
Construct a distance matrix to compare distributions.
Measure Drift:

Compute aggregate statistics like the mean distance, maximum distance, or divergence metrics (e.g., KL divergence).
Detect drift if the distances exceed predefined thresholds.
