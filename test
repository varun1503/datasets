Hyperparameter tuning (HPT) is a crucial step in the machine learning model lifecycle. It aims to identify the optimal set of hyperparameters for a given model, thereby enhancing its overall performance. For this purpose, we utilized Dask and Optuna as frameworks for hyperparameter optimization and model development.

Our work focused on fine-tuning a pre-trained BERT model, customized for a multiclass classification task involving the categories "LIFESTYLE," "MEMBER SERVICE," and "TRAVEL."

Model interpretability (MI) is another vital aspect, especially in regulated industries, as it helps explain model predictions. We explored various algorithms to improve interpretability, including Integrated Gradients and Saliency Maps, to gain deeper insights into model behavior
