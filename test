CMD ["sh", "-c", "./crTo work with your dataset in XLSX format where the columns are text (containing text data) and class (indicating categories such as lifestyle, travel, or member service), you can preprocess it for tasks like classification or drift detection. Below is a step-by-step guide in Python:


---

1. Reading and Exploring the Dataset

First, load your XLSX file into a Pandas DataFrame:

import pandas as pd

# Load dataset
file_path = 'path_to_your_file.xlsx'  # Replace with your file path
df = pd.read_excel(file_path)

# Display first few rows
print(df.head())

# Check for null values
print(df.isnull().sum())

# Classes in the dataset
print("Classes:", df['class'].unique())


---

2. Preprocessing the Dataset

To preprocess:

Encode the classes into numerical values.

Clean the text column (remove special characters, convert to lowercase, etc.).

Split the dataset into training and testing sets.


from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

# Encode classes into numerical values
label_encoder = LabelEncoder()
df['class_encoded'] = label_encoder.fit_transform(df['class'])

# Clean text data (optional: depending on your use case)
def clean_text(text):
    import re
    text = re.sub(r'[^a-zA-Z\s]', '', text)  # Remove special characters
    text = text.lower().strip()             # Convert to lowercase
    return text

df['text_clean'] = df['text'].apply(clean_text)

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    df['text_clean'], df['class_encoded'], test_size=0.2, random_state=42)

print("Training samples:", len(X_train))
print("Testing samples:", len(X_test))


---

3. Creating DataLoaders for PyTorch

If you are using PyTorch, you can create a custom Dataset and DataLoader.

import torch
from torch.utils.data import Dataset, DataLoader

class TextDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts.iloc[idx]
        label = self.labels.iloc[idx]
        encoding = self.tokenizer(
            text,
            max_length=self.max_len,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )
        return {
            'input_ids': encoding['input_ids'].squeeze(0),
            'attention_mask': encoding['attention_mask'].squeeze(0),
            'label': torch.tensor(label, dtype=torch.long)
        }

# Initialize tokenizer (using BERT for example)
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')
max_len = 128

# Create datasets
train_dataset = TextDataset(X_train, y_train, tokenizer, max_len)
test_dataset = TextDataset(X_test, y_test, tokenizer, max_len)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16)


---

4. Model Training and Drift Detection

Now you can train a classifier or perform drift detection.

Example: Drift Detection Using MMDDrift

from alibi_detect.cd import MMDDrift
from functools import partial
from alibi_detect.cd.pytorch import preprocess_drift
from alibi_detect.models.pytorch import TransformerEmbedding

# Initialize Transformer embedding
embedding = TransformerEmbedding('bert-base-cased', emb_type='hidden_state', layers=[-1]).eval()
preprocess_fn = partial(preprocess_drift, model=embedding, tokenizer=tokenizer, max_len=max_len, batch_size=32)

# Prepare reference and test datasets
x_ref = [item['input_ids'].numpy() for item in train_loader]
x_test = [item['input_ids'].numpy() for item in test_loader]

# Create MMDDrift detector
detector = MMDDrift(x_ref, backend='pytorch', p_val=0.05, preprocess_fn=preprocess_fn, n_permutations=1000)

# Predict drift
preds = detector.predict(x_test)
print("Drift detected?" if preds['data']['is_drift'] else "No drift detected")
print(f"p-value: {preds['data']['p_val']:.3f}")


---

5. Next Steps

Text Classification: You can use models like BERT or LSTM for classifying the text into the three categories.

Visualization: Use tools like SHAP or LIME to explain your model predictions.

Evaluation: Evaluate model performance using metrics like precision, recall, F1-score, and accuracy.


Would you like a complete training pipeline for a text classification model?

