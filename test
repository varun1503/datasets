import os
import tempfile
import pandas as pd
from app.utils.elf_logging import logger
from app.chunking.finding_chunking import FindingChunking
from app.utils.s3_storage import S3Utils

REQUIRED_COLS = {"IMVP question", "Finding Impact Category"}

class FindExtraction:
    def __init__(self, message):
        self.message = message

    def s3_object_read(self, s3_file: str) -> str:
        s3_obj = S3Utils()
        temp_dir = tempfile.gettempdir()
        data_dir = os.path.join(temp_dir, "s3_temp_data")
        os.makedirs(data_dir, exist_ok=True)
        local_file_path = os.path.join(data_dir, os.path.basename(s3_file))
        logger.info(f"Downloading from S3: {s3_file} -> {local_file_path}")
        s3_obj.download_file(s3_file, local_file_path)
        if not os.path.exists(local_file_path):
            raise FileNotFoundError(f"S3 download missing: {local_file_path}")
        return local_file_path

    def read_excel(self) -> pd.DataFrame:
        document_file_path = self.s3_object_read(self.message["s3_path"])

        sheet_name = self.message.get("sheet_name", 0)
        # Avoid losing a real first column accidentally
        df = pd.read_excel(document_file_path, sheet_name=sheet_name)  # no index_col

        logger.info(f"Loaded sheet '{sheet_name}' with shape {df.shape}")
        logger.debug(f"Columns: {list(df.columns)}")

        # Normalize column names to avoid whitespace/case issues
        df.columns = [str(c).strip() for c in df.columns]

        missing = REQUIRED_COLS - set(df.columns)
        if missing:
            raise KeyError(
                f"Missing required columns: {missing}. "
                f"Available: {list(df.columns)}"
            )

        df_filter = df[df["IMVP question"].notna()]
        df_filter = df_filter[df_filter["Finding Impact Category"].notna()]
        logger.info(f"Filtered rows: {len(df_filter)}")

        file_name = os.path.basename(document_file_path)
        df_filter["file_name"] = file_name

        if df_filter.empty:
            logger.warning("Filtered DataFrame is empty; nothing to publish.")

        return df_filter


def process_file_finding(processor: FindingChunking, message: dict):
    try:
        ext = os.path.splitext(message.get("file_name", ""))[1].lower()
        if ext not in {".xlsx", ".xls", ".xlsm"}:
            logger.error(f"Unsupported file type: {ext}")
            raise ValueError(f"Unsupported file extension: {ext}")

        # step 1 Load and parse
        try:
            s3_path = message.get("s3_path")
            if not s3_path:
                raise ValueError("s3 file path is not defined")
            retriever = FindExtraction(message)
            logger.info("Reading Excel from S3...")
            data_df = retriever.read_excel()
            logger.info(f"Extraction completed. Shape={data_df.shape}")
        except Exception as e:
            logger.exception("Failed during content preprocessing")
            raise Exception(f"Preprocessing failed: {e}") from e

        # Validate processor + payload before calling publish
        if not hasattr(processor, "publish_message"):
            raise AttributeError("processor has no method publish_message")
        if not isinstance(data_df, pd.DataFrame):
            raise TypeError(f"publish_message expects DataFrame, got {type(data_df)}")

        if data_df.empty:
            logger.warning("DataFrame is empty; publish_message will be skipped.")
            return 0

        try:
            logger.info("Publishing chunks...")
            total_chunks = processor.publish_message(data_df)
            logger.info(f"Chunking completed. Total chunks: {total_chunks}")
            return total_chunks
        except Exception as e:
            logger.exception("Failed during RFR chunking")
            raise Exception(f"RFR chunking failed: {e}") from e

    except Exception as e:
        logger.exception(f"process_file failed for file: {message.get('file_name', 'UNKNOWN')}")
        raise
