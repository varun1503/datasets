# Define baseline (all [PAD] tokens)
    baseline_tokens = ["[PAD]"] * (input_ids.shape[1])  # Same length as input
    baseline_encoding = tokenizer.encode_plus(
        baseline_tokens,
        add_special_tokens=True,
        max_length=128,
        return_token_type_ids=True,
        padding='max_length',
        truncation=True,
        return_attention_mask=True,
        return_tensors='pt'
    )
    
    baseline_input_ids = baseline_encoding['input_ids'].to(device)
    baseline_attention_mask = baseline_encoding['attention_mask'].to(device)
