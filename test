from sklearn.decomposition import PCA
from sklearn.metrics import pairwise_distances
import matplotlib.pyplot as plt
import numpy as np

# Reduce dimensionality with PCA
pca = PCA(n_components=2)  # For visualization, reduce to 2D
ref_pca = pca.fit_transform(ref_embeddings)  # Fit and transform reference embeddings
curr_pca = pca.transform(curr_embeddings)    # Transform current embeddings

# Compute distances from the mean of reference embeddings
mean_ref_pca = ref_pca.mean(axis=0)
distances = pairwise_distances(curr_pca, mean_ref_pca.reshape(1, -1))

# Identify outliers based on a threshold (95th percentile)
threshold = np.percentile(distances, 95)
outliers = [idx for idx, dist in enumerate(distances) if dist > threshold]

# Prepare colors for current data: Red for outliers, Blue for inliers
colors = ["red" if idx in outliers else "blue" for idx in range(len(curr_embeddings))]

# Plot reference and current embeddings
plt.figure(figsize=(10, 6))

# Plot reference embeddings
plt.scatter(ref_pca[:, 0], ref_pca[:, 1], c="green", label="Reference Data", alpha=0.6)

# Plot current embeddings (inliers and outliers)
plt.scatter(curr_pca[:, 0], curr_pca[:, 1], c=colors, label="Current Data", alpha=0.8)

# Add mean reference point
plt.scatter(mean_ref_pca[0], mean_ref_pca[1], c="black", label="Reference Mean", marker="x", s=100)

plt.legend()
plt.title("Outlier Detection in PCA Space")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.grid(True)
plt.show()

# Print results
print("Outlier Indices:", outliers)
print("Outlier Texts:", [current_data[i] for i in outliers])
