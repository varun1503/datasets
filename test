The Model Interpretability (MI) module incorporates multiple algorithms to provide comprehensive insights into model predictions, including LIME, SHAP, Saliency Maps, and Layer Integrated Gradients. Each technique offers a unique perspective on interpretability, as detailed below:

Layer Integrated Gradients: A path attribution explainer that highlights the contributions of specific input features (e.g., words or tokens) to the prediction of a particular class by leveraging the gradients at specific layers of the model.

Saliency Map: A gradient-based method that identifies critical input features (e.g., words or tokens) contributing to the prediction by calculating their attributions.

LIME: A local explainer that interprets individual predictions by perturbing input instances (e.g., masking words) and estimating the contribution of each word to the final prediction.

SHAP: A partition-based explainer that masks subsets of input features (e.g., groups of words) to determine their impact on the model's predictions.

Once the analysis is complete, all visualization plots are saved as artifacts in PNG or JPEG format.






