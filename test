import uuid
import time
import copy
from typing import Dict, List, Any
from app.chunking.automerger import AutoMergeChunker
from aida_genai_core.nlp.remove_pii import remove_pii
from app.utils.elf_logging import logger


class Chunking:
    def __init__(self, producer_utils=None):
        self.auto_merger = AutoMergeChunker()
        self.all_chunks_child = []
        self.hyde_chunks_store = []
        self.separators = ['\\\n\\\n', '\\\n', ' ', '.', '?', '!', ',', ';', ':']
        self.producer_utils = producer_utils

    def create_chunks(self, data: Dict[str, List[Dict[str, Any]]], chunking: bool = True) -> List[Dict[str, Any]]:
        try:
            for filename, pages in data.items():
                if chunking:
                    chunks = self._process_document(pages, filename)
                else:
                    chunks = []
                    for page in pages:
                        doc_id = str(uuid.uuid4())
                        md = page.metadata.copy()
                        md.update({
                            "doc_id": doc_id,
                            "file_name": filename,
                            "level": 2,
                            "type": "doc",
                            "parent_node": None,
                            "prev_node": None,
                            "next_node": None,
                            "child_nodes": None,
                            "chunking_type": "No chunking",
                        })
                        chunks.append({
                            "content": page.page_content,
                            "metadata": md,
                        })

                self.all_chunks_child.extend(self._prepare_datastax_chunks(chunks))

            logger.info(f"Created {len(self.all_chunks_child)} chunks ({'chunked' if chunking else 'un-chunked'})")
            return self.all_chunks_child

        except Exception as e:
            logger.error(f"Error in chunk creation: {e}")
            raise

    def _prepare_datastax_chunks(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        return [
            {
                'content': chunk['content'],
                'metadata': {
                    k: str(v) if not isinstance(v, (str, int, float, bool)) else v
                    for k, v in chunk['metadata'].items()
                }
            }
            for chunk in chunks if chunk['metadata'].get('level') == 2
        ]

    def _process_document(self, pages: List[Dict[str, Any]], filename: str) -> List[Dict[str, Any]]:
        return self.auto_merger._create_chunks(pages)

    def automerger_chunking(self, docs: Dict[str, List[Dict[str, Any]]], message: Dict[str, Any]) -> int:
        chunks = self.create_chunks(data=docs, chunking=True)
        file_name = chunks[0]["metadata"].get("source", "unknown")
        logger.info(f"Processed {file_name}: {len(chunks)} chunks")

        batch = []
        batch_size = 5

        for i, chunk in enumerate(chunks):
            metadata = chunk.get("metadata", {})
            data = {
                "type": metadata.get("type", ""),
                "text": chunk.get("content", ""),
                "chunked_content": chunk.get("content", ""),
                "chunked_content_clean": remove_pii(str(chunk.get("content", ""))),
                "doc_id": metadata.get("doc_id", ""),
                "filename": metadata.get("source", ""),
                "filetype": metadata.get("type", ""),
                "parent_node": metadata.get("parent_node", ""),
                "prev_node": metadata.get("prev_node", ""),
                "level": metadata.get("level", ""),
                "child_nodes": str(metadata.get("child_nodes", "")),
                "next_node": metadata.get("next_node", ""),
                "source": metadata.get("source", ""),
                "period": metadata.get("period", ""),
                "date": metadata.get("date", ""),
                "heading": metadata.get("heading", ""),
                "subheading": metadata.get("subheading", ""),
                "table": metadata.get("table", ""),
                "image_context": metadata.get("image_context", ""),  # no base64
                "tmrc_checklist": metadata.get("tmrc_checklist", ""),
                "section": metadata.get("section", ""),
                "subsection": metadata.get("subsection", ""),
                "caption": metadata.get("caption", ""),
                "style": metadata.get("style", ""),
                "style_extracted": metadata.get("style_extracted", ""),
                "company_name": metadata.get("company_name", ""),
                "start_index": 0,
                "end_index": len(chunk.get("content", "")),
                "doc_format": "docx",
                "input_type": "doc"
            }

            batch.append(data)

            if (i + 1) % batch_size == 0 or (i + 1) == len(chunks):
                try:
                    message_copy = copy.deepcopy(message)
                    message_copy["total_chunks"] = len(chunks)
                    message_copy["chunk_no"] = i + 1
                    message_copy["input_data"] = batch
                    self.producer_utils.publish_message(message_copy)
                    logger.info(f"Published chunk batch {i + 1}/{len(chunks)}")
                    time.sleep(0.2)  # Throttle to reduce CPU spike
                except Exception as e:
                    logger.exception(f"Failed to publish chunk batch {i + 1}: {str(e)}")
                batch = []

        return len(chunks)
