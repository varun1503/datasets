import asyncio
from typing import Sequence, Any

from agent_framework import (
    ChatAgent,
    BaseChatClient,
    MCPStdioTool,
    ChatMessage,
    ChatResponse,
    Role,
    TextContent,
    ToolProtocol,
)

from langchain_core.prompts import ChatPromptTemplate
from safechain.lcel import model


def extract_text_from_message(m: ChatMessage) -> str:
    if m.contents:
        c = m.contents[0]
        return getattr(c, "text", "")
    return ""


def greetings_prompt(user_input: str, time_of_day: str, weather: str) -> str:
    return f"""
You are a friendly greeting assistant.

Time of day: {time_of_day}
Weather info: {weather}

User input:
{user_input}

Respond briefly and politely.
"""


class MyChatClient(BaseChatClient):

    async def _inner_get_response(
        self,
        messages: Sequence[ChatMessage],
        *,
        tools: Sequence[ToolProtocol] | None = None,
        **kwargs: Any,
    ) -> ChatResponse:

        time_of_day = "None"
        weather_info = "None"

        if tools:
            for tool in tools:
                if tool.name == "travel_mcp":
                    time_of_day = await tool.invoke(
                        name="get_time_of_day",
                        arguments={}
                    )
                    weather_info = await tool.invoke(
                        name="get_weather",
                        arguments={"location": "Bangalore"}
                    )

        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    m.role.value,
                    greetings_prompt(
                        extract_text_from_message(m),
                        time_of_day,
                        weather_info,
                    ),
                )
                for m in messages
            ]
        ).format_prompt()

        llm = model("3")
        result = llm.invoke(prompt)

        return ChatResponse(
            messages=ChatMessage(
                role=Role.ASSISTANT,
                contents=[TextContent(text=result.content)],
            )
        )


travel_mcp = MCPStdioTool(
    name="travel_mcp",
    command=["python", "mcp_server.py"],
)


class GreetingAgent(ChatAgent):
    def __init__(self):
        super().__init__(
            name="GreetingAgent",
            chat_client=MyChatClient(),
            instructions="Use local MCP tools when helpful.",
            tools=[travel_mcp],
        )


async def main():
    agent = GreetingAgent()
    response = await agent.run("Hi, how is the weather?")
    print("\nFINAL RESPONSE:\n")
    print(response.text)def extract_text_from_message(m: ChatMessage) -> str:
    """Extract text content from a ChatMessage."""
    if hasattr(m, "contents") and m.contents:
        c = m.contents[0]
        if hasattr(c, "text"):
            return c.text
        if hasattr(c, "value"):
            return c.value
        if hasattr(c, "data"):
            return c.data
    return ""


def greetings_prompt(user_input: str, time_of_day: str, weather: str) -> str:
    """Generate greeting prompt with context."""
    return f"""
You are a friendly, polite greeting assistant.

Time of day: {time_of_day}
Weather info: {weather}

Greet the user appropriately using the time and weather.

User input:
{user_input}

Keep the response short and friendly.
"""


def _merge_chat_options(
    *,
    base_chat_options: ChatOptions | Any | None = None,
    model_id: str | None = None,
    allow_multiple_tool_calls: bool | None = None,
    frequency_penalty: float | None = None,
    logit_bias: dict[str | int, float] | None = None,
    max_tokens: int | None = None,
    metadata: dict[str, Any] | None = None,
    presence_penalty: float | None = None,
    response_format: type[BaseModel] | None = None,
    seed: int | None = None,
    stop: str | Sequence[str] | None = None,
    store: bool | None = None,
    temperature: float | None = None,
    tool_choice: ToolMode | Literal["auto", "required", "none"] | dict[str, Any] | None = None,
    tools: list[ToolProtocol | dict[str, Any] | Callable[..., Any]] | None = None,
    top_p: float | None = None,
    user: str | None = None,
    additional_properties: dict[str, Any] | None = None,
) -> ChatOptions:
    """Merge base chat options with direct parameters to create a new ChatOptions instance."""
    if base_chat_options is not None and not isinstance(base_chat_options, ChatOptions):
        raise TypeError("chat_options must be an instance of ChatOptions")

    if base_chat_options is None:
        base_chat_options = ChatOptions()

    return base_chat_options & ChatOptions(
        model_id=model_id,
        allow_multiple_tool_calls=allow_multiple_tool_calls,
        frequency_penalty=frequency_penalty,
        logit_bias=logit_bias,
        max_tokens=max_tokens,
        metadata=metadata,
        presence_penalty=presence_penalty,
        response_format=response_format,
        seed=seed,
        stop=stop,
        store=store,
        temperature=temperature,
        top_p=top_p,
        tool_choice=tool_choice,
        tools=tools,
        user=user,
        additional_properties=additional_properties,
    )


# =========================================================
# CHAT CLIENT WITH MCP - WINDOWS COMPATIBLE
# =========================================================
class MyChatClientWithMCP(BaseChatClient):
    def __init__(self):
        super().__init__()
        self.mcp_session: ClientSession | None = None
        self.mcp_exit_stack = None
        
    async def connect_mcp(self):
        """Connect to the local MCP server."""
        # Determine Python executable
        python_cmd = sys.executable
        
        # For Windows, we need to use different approach
        is_windows = platform.system() == "Windows"
        
        if is_windows:
            # On Windows, use pythonw or python with creationflags
            server_params = StdioServerParameters(
                command=python_cmd,
                args=["mcp_server.py"],
                env=None
            )
        else:
            server_params = StdioServerParameters(
                command=python_cmd,
                args=["mcp_server.py"],
                env=None
            )
        
        try:
            # Use AsyncExitStack to manage context
            self.mcp_exit_stack = AsyncExitStack()
            
            # Enter the stdio_client context
            stdio_transport = await self.mcp_exit_stack.enter_async_context(
                stdio_client(server_params)
            )
            stdio, write = stdio_transport
            
            # Initialize MCP session
            self.mcp_session = ClientSession(stdio, write)
            await self.mcp_session.initialize()
            
            # List available tools
            tools_list = await self.mcp_session.list_tools()
            print(f"‚úÖ Connected to MCP. Tools: {[t.name for t in tools_list.tools]}")
            
        except Exception as e:
            print(f"‚ùå Failed to connect to MCP: {e}")
            print(f"Platform: {platform.system()}")
            raise
        
    async def call_mcp_tool(self, tool_name: str, arguments: dict = None) -> str:
        """Call a tool via MCP."""
        if not self.mcp_session:
            raise RuntimeError("MCP session not initialized. Call connect_mcp() first.")
        
        result = await self.mcp_session.call_tool(tool_name, arguments or {})
        
        # Extract text from result
        if result.content and len(result.content) > 0:
            return result.content[0].text
        return ""
    
    async def _inner_get_response(
        self,
        messages: Sequence[ChatMessage],
        *,
        tools: Sequence[ToolProtocol] | None = None,
        tool_choice: Callable[..., Any] | None = None,
        chat_options: MutableMapping[str, Any] | None = None,
        **kwargs: Any,
    ) -> ChatResponse:

        # Ensure MCP connection
        if not self.mcp_session:
            await self.connect_mcp()

        # Initialize with defaults
        time_of_day = "unknown"
        weather_info = "unavailable"
        
        # Extract user message
        user_message = extract_text_from_message(messages[-1])
        
        # Call MCP tools
        try:
            print("üîß Calling get_time_of_day via MCP...")
            time_of_day = await self.call_mcp_tool("get_time_of_day")
            print(f"‚úÖ Time of day: {time_of_day}")
            
            print("üîß Calling get_weather via MCP...")
            weather_info = await self.call_mcp_tool("get_weather", {"location": "Bangalore"})
            print(f"‚úÖ Weather: {weather_info}")
            
        except Exception as e:
            print(f"‚ùå MCP tool execution error: {e}")
            import traceback
            traceback.print_exc()

        # Build prompt with tool results
        prompt_content = greetings_prompt(
            user_message,
            time_of_day=time_of_day,
            weather=weather_info,
        )
        
        prompt_template = ChatPromptTemplate.from_messages([
            (messages[-1].role.value, prompt_content)
        ])

        prompt_value = prompt_template.format_prompt()
        llm = model("3")
        llm_response = llm.invoke(prompt_value)

        response_message = ChatMessage(
            role=Role.ASSISTANT,
            contents=[TextContent(text=llm_response.content)],
        )

        return ChatResponse(messages=response_message)

    async def _inner_get_streaming_response(
        self,
        messages: Sequence[ChatMessage],
        *,
        tools: Sequence[ToolProtocol] | None = None,
        tool_choice: Callable[..., Any] | None = None,
        chat_options: MutableMapping[str, Any] | None = None,
        **kwargs: Any,
    ) -> AsyncIterable[ChatResponse]:
        raise NotImplementedError("Streaming not implemented")
    
    async def cleanup(self):
        """Cleanup MCP connection."""
        if self.mcp_exit_stack:
            try:
                await self.mcp_exit_stack.aclose()
                print("‚úÖ MCP connection closed")
            except Exception as e:
                print(f"‚ö†Ô∏è Error during cleanup: {e}")


class GreetingAgentWithMCP(ChatAgent):
    def __init__(self):
        chat_client = MyChatClientWithMCP()
        super().__init__(
            name="GreetingAgentMCP",
            chat_client=chat_client,
            instructions="You are a friendly greeting assistant with MCP tools.",
        )
    
    async def cleanup(self):
        """Cleanup resources."""
        await self.chat_client.cleanup()


async def main():
    # Set event loop policy for Windows
    if platform.system() == "Windows":
        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())
    
    agent = GreetingAgentWithMCP()
    
    try:
        print("üöÄ Starting greeting agent with MCP...\n")
        print(f"Platform: {platform.system()}")
        print(f"Python: {sys.version}\n")
        
        response = await agent.run("Hi there!")
        print("\n‚ú® FINAL RESPONSE:")
        print(response.text)
    except Exception as e:
        print(f"\n‚ùå Error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        await agent.cleanup()
        print("\nüëã Cleaned up MCP connection")


if __name__ == "__main__":
    asyncio.run(main())
```

## Key Changes for Windows Compatibility:

1. **Added platform detection**: `platform.system() == "Windows"`
2. **Set Windows event loop policy**: `asyncio.WindowsProactorEventLoopPolicy()`
3. **Better error handling**: Added try-catch blocks with traceback
4. **Used `sys.executable`**: To get the correct Python path

## Alternative: If the above still doesn't work on Windows

If you still get the `fileno` error, it's likely that the MCP stdio transport doesn't fully support Windows. In that case, use **HTTP-based MCP** instead:

Would you like me to provide the HTTP-based MCP version which is more Windows-compatible?
