def invoke_batch(self, payload_list: List[Dict], batch_size: int = 5) -> List[Dict]:
    """
    Invoke LLM calls in batches of batch_size using multithreading for each batch.
    """
    results = []
    total = len(payload_list)

    for i in range(0, total, batch_size):
        batch = payload_list[i:i + batch_size]
        logger.info(f"Invoking batch {i // batch_size + 1} of {((total - 1) // batch_size) + 1}")
        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = [executor.submit(self._llm_single_call, payload) for payload in batch]
            for future in as_completed(futures):
                results.append(future.result())

    return results
def enrich_chunks_with_llm(preprocessed_chunks: List[Dict], prompt_image: str, max_workers: int = 5) -> List[Dict]:
    llm_batcher = BatchedLLMInvoker(max_workers=max_workers)
    payloads = []
    chunk_image_map = []

    for idx, chunk in enumerate(preprocessed_chunks):
        for base64_image in chunk.get("image_context", []):
            payloads.append({
                "model_index": "gpt-40",
                "prompt": prompt_image,
                "input_nodes": {"base_64_encoded_image": base64_image}
            })
            chunk_image_map.append((idx, base64_image))

    results = llm_batcher.invoke_batch(payloads, batch_size=5)

    from collections import defaultdict
    chunk_image_outputs = defaultdict(list)
    for (chunk_idx, base64_img), result in zip(chunk_image_map, results):
        chunk_image_outputs[chunk_idx].append({
            "base64": base64_img,
            "llm_output": result.get("output", "None")
        })

    for idx, enriched_images in chunk_image_outputs.items():
        preprocessed_chunks[idx]["image_context"] = enriched_images

    return preprocessed_chunks
