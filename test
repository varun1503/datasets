import streamlit as st
import requests
import os

# Base URL of your FastAPI server
API_BASE_URL = "http://127.0.0.1:8080"  # Update if deployed elsewhere

st.set_page_config(page_title="RAG App", layout="wide")
st.title("RAG Application")

# -------------------------
# 1. Upload Document Section
# -------------------------
st.header("1. Upload Document")

uploaded_file = st.file_uploader("Upload a file (pdf, txt, docx)", type=["pdf", "txt", "docx"])
upload_index_id = st.text_input("Index ID", value="df101")
chunking_type = st.selectbox("Select Chunking Type", ["recursive", "semantic", "automerger"])

document_path = None

if uploaded_file:
    with open(uploaded_file.name, "wb") as f:
        f.write(uploaded_file.getbuffer())
    document_path = os.path.abspath(uploaded_file.name)

if st.button("Upload and Chunk Document"):
    if document_path and upload_index_id:
        request_payload = {
            "document_path": document_path,
            "chunking_type": chunking_type,
            "index_id": upload_index_id
        }
        response = requests.post(f"{API_BASE_URL}/upload/documents", json=request_payload)
        if response.status_code == 200:
            st.success("Document uploaded and chunked successfully!")
        else:
            st.error(f"Failed to upload document: {response.text}")
    else:
        st.warning("Upload a document and provide an index ID.")

# -------------------------
# 2. Retrieval Chat Section
# -------------------------
st.header("2. RAG Chat Interface")

# Init session state for chat
if "messages" not in st.session_state:
    st.session_state.messages = []

# Retrieval input settings
index_id = st.text_input("Faiss Index ID", value="df101", key="chat_index")
generative_model_id = st.selectbox("Model Index ID", options=["3", "1"], key="chat_model")
top_k = st.slider("Number of documents to retrieve (Top K)", 1, 10, 5, key="chat_topk")

# Chat layout: chat left, docs right
chat_col, doc_col = st.columns([2, 1])

with chat_col:
    st.subheader("Chat History")
    with st.container():
        for msg in st.session_state.messages:
            if msg["role"] == "user":
                st.markdown(f"**You:** {msg['content']}")
            else:
                st.markdown(f"**Assistant:** {msg['content']}")

# Chat input at bottom
user_input = st.chat_input("Ask your question")

# On message submission
if user_input:
    st.session_state.messages.append({"role": "user", "content": user_input})

    # Make retrieval call
    if index_id:
        request_payload = {
            "query": user_input,
            "top_k": top_k,
            "index_id": index_id,
            "generative_model_id": generative_model_id
        }

        try:
            response = requests.post(f"{API_BASE_URL}/retrieve_and_generate", json=request_payload)
            if response.status_code == 200:
                result = response.json()
                answer = result.get("generated_response", "No response.")
                st.session_state.messages.append({"role": "assistant", "content": answer})

                with chat_col:
                    st.markdown(f"**Assistant:** {answer}")

                with doc_col:
                    st.subheader("Retrieved Documents")
                    for doc in result.get("retrieved_documents", []):
                        st.markdown(f"**Page {doc['page']} | Source: {doc['source']}**")
                        st.write(doc["content"])
                        st.markdown("---")
            else:
                st.error(f"Error from server: {response.text}")
        except Exception as e:
            st.error(f"Request failed: {e}")
    else:
        st.warning("Please provide an Index ID to query.")
