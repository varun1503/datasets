# basic_checks.py
# pip install fuzzysearch fuzzywuzzy

from __future__ import annotations
from dataclasses import dataclass, field
from typing import List, Dict, Pattern, Tuple, Optional
import os
import re

from fuzzywuzzy import fuzz
from fuzzysearch import find_near_matches


@dataclass
class BasicChecks:
    # Tunables
    max_l_dist: int = 1
    decline_lead_1: str = "Here's the reason we made the decision"
    decline_lead_2: str = "Reason(s) for Our Decision"
    ecoa_string: str = "The federal Equal Credit Opportunity Act"
    continued_filler_next: str = "Continued  on  next  page"

    credit_bureau_list: List[str] = field(default_factory=lambda: [
        "Experian",
        "D&B corporation",
        "TransUnion Consumer Relations",
        "LexisNexis Risk Solutions Bureau LLC"
    ])

    # Regexes
    date_regex: Pattern[str] = re.compile(
        r"(?:January|February|March|April|May|June|July|August|September|October|November|December)"
        r"\s+(?:[1-9]|[12]\d|3[01]),?\s+\d{4}"
    )

    # ----------------------- helpers -----------------------

    @staticmethod
    def sentence_split(text: str) -> List[str]:
        """Very literal splitter like your original approach."""
        out, cur = [], []
        for ch in text:
            cur.append(ch)
            if ch in [".", ",", "!", "\n", "?"]:
                out.append("".join(cur))
                cur = []
        if cur:
            out.append("".join(cur))
        return out

    @staticmethod
    def slice_next_sentences(content: str, start_idx: int, n_sentences: int) -> str:
        """Return a block consisting of the next N sentences starting at start_idx."""
        if start_idx < 0 or start_idx >= len(content):
            return ""
        tail = re.sub(r"[ \t]+", " ", content[start_idx:])
        parts = re.split(r"(?<=[\.!?])\s+", tail)
        return " ".join(parts[:max(1, n_sentences)]).strip()

    @staticmethod
    def list_file_paths(folder_path: str) -> List[str]:
        """Recursive file lister (all files, absolute paths)."""
        acc = []
        for root, _, files in os.walk(folder_path):
            for f in files:
                acc.append(os.path.abspath(os.path.join(root, f)))
        return acc

    # ----------------------- single-field extractors -----------------------

    def find_action(self, content: str, threshold: int = 50) -> Optional[Tuple[str, int]]:
        """
        Fuzzy scan for a sentence near: 'unable cannot approve your at this time'
        Returns (matched_sentence, ratio) or None.
        """
        target = "unable cannot approve your at this time"
        for sent in self.sentence_split(content):
            if fuzz.ratio(target, sent) > threshold:
                return sent.strip(), fuzz.ratio(target, sent)
        return None

    def find_decline_reason(self, content: str) -> str:
        """
        Look for either lead sentence and return a multi-sentence snippet.
        Your original used 'find_item'/'find_item2'; we approximate with 12â€“15 sentences.
        """
        r1 = find_near_matches(self.decline_lead_1, content, max_l_dist=self.max_l_dist)
        r2 = find_near_matches(self.decline_lead_2, content, max_l_dist=self.max_l_dist)
        if not r1 and not r2:
            return "not found"
        start = (r1 or r2)[0].start
        # Slightly longer window to be safe
        return self.slice_next_sentences(content, start, 15)

    def find_ecoa(self, content: str) -> str:
        hits = find_near_matches(self.ecoa_string, content, max_l_dist=self.max_l_dist)
        if not hits:
            return "not found"
        s = hits[0].start
        block = self.slice_next_sentences(content, s, 10)
        if self.continued_filler_next in block:
            block = self.slice_next_sentences(content, s, 17)
        return block

    def find_federal_agency(self, content: str) -> str:
        phrase = "The federal agency that administers compliance with this law concerning"
        hits = find_near_matches(phrase, content, max_l_dist=self.max_l_dist)
        if not hits:
            return "not found"
        s = hits[0].start
        return self.slice_next_sentences(content, s, 3)

    @staticmethod
    def should_check_fico(text: str, dcsn_rsn_cd: str) -> bool:
        """
        Your original referenced 'content' by mistake; fixed to use 'text'.
        Rule: check if 'FICO score' present OR decision code == 'd2f'.
        """
        return ("FICO score" in text) or (dcsn_rsn_cd.lower() == "d2f")

    def get_credit_bureaus(self, content: str) -> List[str]:
        found = []
        for item in self.credit_bureau_list:
            if item in content:
                found.append(item)
        return found

    def find_credit_bureau(self, content: str) -> List[str]:
        return self.get_credit_bureaus(content)

    def find_second_date_if_bureau(self, content: str) -> str:
        """
        If a bureau is present and we find 2+ dates, return the 2nd; else 'not found'.
        """
        if not self.get_credit_bureaus(content):
            return "not found"
        dates = re.findall(self.date_regex, content)
        if len(dates) <= 1:
            return "not found"
        return dates[1]

    def find_fico_score(self, content: str) -> str:
        """
        Gate on bureau presence (like your code) and extract next ~3 sentences.
        """
        if not self.get_credit_bureaus(content):
            return "not found (no bureau)"
        hits = find_near_matches("FICO score was", content, max_l_dist=self.max_l_dist)
        if not hits:
            return "not found"
        s = hits[0].start
        return self.slice_next_sentences(content, s, 3)

    def find_factors(self, content: str) -> str:
        """
        Gate on bureau presence. Your phrase looked misspelled; keeping as-is.
        """
        if not self.get_credit_bureaus(content):
            return "not found (no bureau)"
        phrase = "the key factors that contrubuted to your FICO score"
        hits = find_near_matches(phrase, content, max_l_dist=self.max_l_dist)
        if not hits:
            return "not found"
        s = hits[0].start
        return self.slice_next_sentences(content, s, 8)

    # ----------------------- per-file wrapper -----------------------

    def analyze_file(self, file_path: str, dcsn_rsn_cd: str = "") -> Dict[str, str]:
        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            content = f.read()

        action_hit = self.find_action(content)
        decline = self.find_decline_reason(content)
        ecoa = self.find_ecoa(content)
        agency = self.find_federal_agency(content)
        bureaus = self.find_credit_bureau(content)
        date2 = self.find_second_date_if_bureau(content)
        fico_score = self.find_fico_score(content) if self.should_check_fico(content, dcsn_rsn_cd) else "skipped"
        factors = self.find_factors(content)

        return {
            "Action Sentence": f"{action_hit[0]}  (ratio={action_hit[1]})" if action_hit else "not found",
            "Decline Reason": decline,
            "ECOA Section": ecoa,
            "Federal Agency": agency,
            "Credit Bureaus": ", ".join(bureaus) if bureaus else "not found",
            "2nd Date (if bureau)": date2,
            "FICO Score": fico_score,
            "FICO Factors": factors,
        }

    # ----------------------- batch runner -----------------------

    def run(self, file_paths_txt: List[str], output_txt_path: str | None = None, dcsn_rsn_cd: str = "") -> Dict[str, Dict[str, str]]:
        """
        Process many text files; optionally write a text report.
        Returns: {file_path: {field: value}}
        """
        results: Dict[str, Dict[str, str]] = {}
        lines: List[str] = []

        for fp in file_paths_txt:
            try:
                info = self.analyze_file(fp, dcsn_rsn_cd=dcsn_rsn_cd)
                results[fp] = info

                lines.append(f"FILE: {fp}")
                for k, v in info.items():
                    lines.append(f"{k}: {v}")
                lines.append("-" * 96)
            except Exception as e:
                results[fp] = {"ERROR": str(e)}
                lines.append(f"FILE: {fp}\nERROR: {e}\n{'-'*96}")

        if output_txt_path:
            os.makedirs(os.path.dirname(os.path.abspath(output_txt_path)), exist_ok=True)
            with open(output_txt_path, "w", encoding="utf-8") as out:
                out.write("\n".join(lines))

        return results


# ----------------------------- example usage -----------------------------
if __name__ == "__main__":
    folder = ""  # directory containing your .txt letters
    files = BasicChecks.list_file_paths(folder)
    out_txt = ""  # e.g., r"C:\tmp\basic_checks_report.txt" (leave "" to skip writing)

    checker = BasicChecks(max_l_dist=1)
    report = checker.run(files, output_txt_path=out_txt or None, dcsn_rsn_cd="d2f")

    # Quick console summary
    for fp, fields in report.items():
        print("\n", fp)
        for k, v in fields.items():
            print(f"  - {k}: {v}")
