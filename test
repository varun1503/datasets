from cassandra.cluster import Cluster
from cassandra.auth import PlainTextAuthProvider
from cassandra.query import BatchStatement, SimpleStatement
from tqdm import tqdm
import uuid
from datetime import datetime, timezone

# Initialize Cassandra cluster and session
auth_provider = PlainTextAuthProvider(username=USER, password=PASSWORD)
cluster = Cluster(cluster1_nodes, auth_provider=auth_provider)
session = cluster.connect(keyspace)

# Prepare the insert query
insert_query = session.prepare(f"""
    INSERT INTO {table} (
        file_name,
        row_id,
        chunk_text,
        created_ts,
        metadata,
        use_case_id,
        use_case_name,
        embedding
    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
""")

batch = BatchStatement()
batch_size = 50  # Tune this based on your cluster's capacity

for i, data_point in enumerate(tqdm(chunk_data[:5], total=len(chunk_data), desc="inserting data")):
    metadata = data_point.get("metadata", {})
    print("Raw metadata:", metadata)

    meta_data = {
        "parent_node": [metadata.get("parent_node")] if metadata.get("parent_node") else ["NA"],
        "doc_id": [metadata.get("doc_id")],
        "prev_node": [metadata.get("prev_node")] if metadata.get("prev_node") else ["NA"],
        "child_nodes": metadata.get("child_nodes") if metadata.get("child_nodes") else ["NA"],
        "next_node": [metadata.get("next_node")] if metadata.get("next_node") else ["NA"],
        "period": [metadata.get("period")],
        "type": [metadata.get("type")],
        "chunking_type": [metadata.get("chunking_type")],
        "company_name": [metadata.get("company_name")],
        "source_content": metadata.get("source_content")
    }

    row_id = str(uuid.uuid4())
    chunk_text = data_point["content"]
    embedding = data_point["embedding"][0]
    file_name = metadata.get("source", "unknown")
    created_ts = datetime.now(timezone.utc)
    use_case_id = 101
    use_case_name = "Earnings Call Analysis"

    batch.add(insert_query, (file_name, row_id, chunk_text, created_ts, meta_data, use_case_id, use_case_name, embedding))

    if (i + 1) % batch_size == 0 or i == len(chunk_data[:5]) - 1:
        try:
            session.execute(batch)
            batch.clear()
        except Exception as e:
            print(f"Error inserting batch at index {i}: {e}")

# Shutdown
cluster.shutdown()
