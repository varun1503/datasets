from transformers import BertTokenizer
from sklearn.feature_extraction.text import TfidfVectorizer
import numpy as np

# Load BERT tokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")

# Tokenize texts
def tokenize_texts(texts):
    return [" ".join(tokenizer.tokenize(text)) for text in texts]

# Compute TF or TF-IDF values
def compute_tfidf(texts):
    tfidf_vectorizer = TfidfVectorizer()
    tfidf_matrix = tfidf_vectorizer.fit_transform(texts)
    return tfidf_matrix.toarray(), tfidf_vectorizer.get_feature_names_out()

# Tokenize and compute TF-IDF for reference and current datasets
reference_tokens = tokenize_texts(reference_texts)
current_tokens = tokenize_texts(current_texts)

reference_tfidf, token_names = compute_tfidf(reference_tokens)
current_tfidf, _ = compute_tfidf(current_tokens)
